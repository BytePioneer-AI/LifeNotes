[{"date":"2025-09-02","title":"腾讯混元 Hunyuan-MT-7B","text":"**摘要：** 腾讯开源的70亿参数轻量级翻译大模型，在WMT25国际机器翻译比赛中斩获30个语言方向的第一名，实现了“小参数量，大能量”的技术突破，并配套开源了业界首个翻译集成模型 `Hunyuan-MT-Chimera-7B`。\n\n**模型规格与架构：**\n- **参数量：** 7B\n- **支持语言：** 支持33种语言互译，并对5种中国少数民族语言/方言（如藏语、维吾尔语等）进行了优化。\n- **训练范式：** 采用创新的“预训练 → 跨语言预训练(CPT) → 监督微调 → 翻译强化 → 集成强化”五阶段训练流程，通过跨语言对齐机制提升了低资源语言的翻译质量。\n- **配套模型：** 开源了业界首个翻译集成模型 `Hunyuan-MT-Chimera-7B`，可融合多个翻译模型（如 `Hunyuan-MT-7B`、`deepseek` 等）的结果，生成更优译文。\n\n**性能与实测表现：**\n- **竞赛基准：** 在WMT25国际机器翻译比赛中，获得了全部31个语种中的30个第一名，处于绝对领先地位。\n- **公开基准：** 在Flores200评测集上，BLEU评分平均达到42.3，表现优于同尺寸模型，且不逊于部分超大尺寸闭源模型。\n- **实测效果：** 在俚语、古诗词、网络缩写等复杂场景下表现出色，翻译结果兼顾“信达雅”。\n\n**优势：**\n- **轻量化高性能：** 以7B的较小参数量实现了与超大模型相媲美的性能，证明了精妙架构设计的有效性。\n- **计算效率高：** 推理速度快，通过腾讯自研的 `AngelSlim` 压缩工具进行FP8量化后，推理性能可进一步提升30%，能在消费级GPU上流畅运行。\n- **部署友好：** 对硬件要求低，部署、运行和维护成本相对更低，为中小企业降低了AI应用门槛。\n- **开源生态完善：** 模型、技术报告以及配套的压缩工具均已开源，方便社区开发者使用和二次创新。\n\n**适用场景 / 备注：**\n- 已在腾讯会议、企业微信、QQ浏览器等多个内部业务中落地。\n- `Hunyuan-MT-Chimera-7B` 集成模型特别适用于对准确性要求极高的专业领域，如法律、医疗文本翻译。","modelSize":"7B","modelType":"语言","openSource":true,"contextWindow":"","officialDoc":"https://github.com/Tencent-Hunyuan/Hunyuan-MT/"},{"date":"2025-08-31","title":"美团 LongCat-Flash-Chat","text":"**摘要：** 美团于2025年8月31日开源的混合专家（MoE）大语言模型，以其创新的动态计算架构为核心，实现了极高的推理速度（>100 tokens/s）和极具竞争力的成本（$0.7/百万输出token），标志着业界向更实用、高效模型发展的又一重要里程碑。\n\n**模型规格与架构：**\n- **参数量：** 总参数量为`5600亿`，根据上下文动态激活`186亿`至`313亿`（平均约`270亿`）。\n- **架构/范式：** 采用创新的混合专家（MoE）架构，包含两大亮点：\n  - **零计算专家 (Zero-computation Experts)：** 引入了256个不消耗算力的专家，让模型可以为简单的Token（如标点、连词）分配更少或零计算资源，实现按需计算。\n  - **快捷连接MoE (Shortcut-connected MoE, ScMoE)：** 通过跨层快捷连接，将前一层的计算与当前MoE层的通信并行执行，极大地提升了训练和推理的系统吞吐量。\n- **上下文：** 训练时将上下文长度扩展至`128K`。\n\n**性能与实测表现：**\n- **基准：** 作为一个非思考型模型，其综合性能与 `DeepSeek-V3.1` 和 `Kimi-K2` 等领先模型相当，尤其在智能体（Agent）工具使用方面经过专门优化，表现出色。\n- **实测：**\n  - **速度与成本：** 社区实测普遍证实其极快的响应速度和极低的首Token延迟，成本优势明显。\n  - **强项：** 指令遵循效果良好，代码生成能力表现不俗。\n  - **弱项：** 在需要深度背景知识的复杂推理任务上表现不佳（例如无法理解“但丁/丁真”的语言梗，或未能推理出“老鹰本身就会飞”），长数学题的解答过程可能被截断或末尾出错。\n\n**优势：**\n- **极致的性能效率：** 推理速度极快，延迟极低，运营成本极具竞争力。\n- **架构创新：** `零计算专家`和`ScMoE`为高效大模型的设计提供了有价值的新思路。\n- **强大的系统优化：** 从训练到推理都进行了深度的系统级协同优化，保证了高稳定性和高效率。\n\n**劣势：**\n- **复杂推理能力有限：** 与顶级的闭源模型相比，在知识深度和复杂逻辑推理链上仍有差距。\n- **长文本生成稳定性：** 在处理超长或需要多步精确计算的生成任务时，输出的完整性和准确性有时会下降。","modelSize":"560B (总计 560B, 激活 18.6B~31.3B)","modelType":"语言, 代码","openSource":true,"contextWindow":"128K","officialDoc":"https://github.com/meituan-longcat/LongCat-Flash-Chat"},{"date":"2025-08-27","title":"⭐Google Gemini 2.5 Flash Image (nano-banana)","text":"**摘要：** Google发布的顶级图像生成与编辑模型，在LMArena盲测中以创纪录优势夺冠。该模型以其卓越的角色一致性、强大的提示编辑能力和极高的性价比著称，但在处理复杂任务时稳定性有待提高。\n\n**模型规格与架构：**\n- 官方名称：`Gemini 2.5 Flash Image`，社区昵称 `nano-banana`。\n- 知识截止：2025年6月。\n- 性能：生成速度快，API每分钟最多可调用500次。\n- 成本：定价极低，每张图片成本约0.039美元（人民币不到3毛钱），性价比极高。\n\n**性能与实测表现：**\n- **基准测试**：在LMArena盲测竞技场中获得历史最高Elo分数领先优势，被广泛认为是当前最强的图像编辑模型。\n- **角色一致性 **：核心优势。能够跨多张图片高度保持人物的面部、神态和穿着，效果自然，AI味少。偶有小瑕疵（如生成两副墨镜）。\n- **图像编辑**：表现顶尖。可通过简单的自然语言提示完成精确的局部修改（如修复“六指图”）、无缝添加元素（如纹身），无需手动框选。\n- **风格迁移**：对主流艺术风格（如油画、动漫）的迁移效果出色，但处理小众风格（如像素风）时表现较弱。\n- **多元素融合与复杂编辑**：最主要的短板。当提示包含多个指令（如同时换装、换背景、改动作）时，稳定性急剧下降，成功率低，需要用户多次尝试。\n\n**优势：**\n- SOTA级别的角色一致性与图像编辑能力，效果逼真自然。\n- 极具竞争力的价格和快速的生成速度，性价比极高。\n- 创意生成能力强，适合制作手办场景等复杂有趣的视觉内容。\n- 在社区评测中，综合表现被认为优于 `GPT-4o` 等竞争对手。\n\n**劣势：**\n- 稳定性不足，出图质量波动较大，完美结果需要多次生成（“roll图”）。\n- 难以胜任复杂、多元素的编辑任务，指令越复杂，失败率越高。\n\n**适用场景 / 备注：**\n- 适用于需要保持角色连贯性的故事叙述、营销内容创作和概念设计。\n- 作为高效、低成本的P图工具，可用于快速修复或创意修改图片。\n- 测评主要围绕其核心更新点“以图生图”和图片编辑展开。","modelSize":"","modelType":"图像生成, 多模态","openSource":false,"contextWindow":"","officialDoc":"https://developers.googleblog.com/en/introducing-gemini-2-5-flash-image/","evaluation":"又不需要PS了？\n实测后，确实🐂，图片修改比图片生成好。"},{"date":"2025-08-26","title":"⭐面壁智能 MiniCPM-V 4.5","text":"**摘要：** 面壁智能开源的8B端侧多模态旗舰模型，是行业首个具备“高刷”视频理解能力的模型，在多项评测中性能越级超越`Qwen2.5-VL 72B`等更大规模模型。\n\n**模型规格与架构：**\n- **参数量：** `8B`\n- **架构/范式：** 基于`Qwen3-8B`和`SigLIP2-400M`构建，核心创新是`3D-Resampler`结构，可进行三维视频片段的高密度压缩。\n\n**性能与实测表现：**\n- **基准：** 在`OpenCompass`、`MotionBench`、`FavorBench`、`LVBench`、`OmniDocBench`等多个图文、视频理解榜单上取得同级SOTA，并超越部分更大模型。\n- **实测：** 视频处理可实现高达`10fps`的抽帧，在同等视觉Token开销下可接收6倍视频帧，视觉压缩率达96倍。在`Video-MME`评测上，推理时间开销仅为同级模型的1/10。\n\n**优势：**\n- **高刷视频理解：** 行业首创，能更精细地捕捉动态细节，将视频理解从“看PPT”提升到“看动态画面”。\n- **越级性能：** 以`8B`参数量在图片理解、视频理解、文档解析等核心任务上全面超越`72B`规模的模型，实现“以小博大”。\n- **端侧友好：** 推理效率极高，显存占用低、速度快，是端侧AI应用的优选方案。\n- **灵活推理：** 支持“常规模式”和“深度思考模式”，可根据任务复杂性平衡性能与响应速度。\n\n**适用场景：**\n- 需要精细动态捕捉的视频分析、移动端实时多模态交互、复杂文档自动化处理等。","modelSize":"8B","modelType":"多模态, 文档解析模型","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/openbmb/MiniCPM-V-4_5","evaluation":"发现一个好现象：国内的AI‘六小龙’不再只卷通用大模型了，很多开始深耕特定领域，通过聚焦资源在特定赛道上构建核心竞争力。在我们的近期测试中，对于检验单的识别效果比Qwen-VL 72B效果要好。"},{"date":"2025-08-26","title":"书生·InternVL 3.5","text":"**摘要：** InternVL 3.5 是由上海人工智能实验室发布的开源多模态大模型系列，通过级联强化学习（Cascade RL）、视觉分辨率路由器（ViR）和解耦部署（DvD）等创新，显著提升了模型的推理能力、效率和多功能性。其旗舰模型在多个基准上达到了开源社区的领先水平，并缩小了与顶尖商业模型的性能差距。\n\n**模型规格与架构：**\n- **模型系列：** 共发布9个模型，涵盖密集型（Dense）和专家混合（MoE）两种架构。\n  - **Dense模型：** `1B`、`2B`、`4B`、`8B`、`14B`、`38B`\n  - **MoE模型：** `InternVL3.5-20B-A4B`、`InternVL3.5-30B-A3B`、`InternVL3.5-241B-A28B`\n- **架构/范式：** 延续了 `ViT–MLP–LLM` 范式，视觉编码器采用 `InternViT-300M` 和 `InternViT-6B`，语言模型基于 `Qwen3` 和 `GPT-OSS` 系列初始化。\n- **高效变体：** 引入 `ViR` 机制的模型被称为 `InternVL3.5-Flash`，专为资源受限场景优化。\n\n**性能与实测表现：**\n- **推理能力：** 相较于前代 `InternVL 3`，推理性能平均提升超过10个百分点，在 `MMMU` 和 `MathVista` 等复杂推理基准上进步显著。\n- **综合性能：** 旗舰模型 `InternVL3.5-241B-A28B` 在通用多模态、推理、文本和代理任务的综合评测中，取得了开源模型的最佳结果，其通用多模态能力与 `GPT-5` 相当。\n- **效率提升：** `InternVL3.5-Flash` 能在维持近100%性能的同时，将视觉令牌数量减少50%。`DvD` 部署策略使推理速度提升4.05倍。\n- **新功能：** 扩展了对GUI交互、具身代理和SVG理解与生成等新能力的支持，并在 `OSWorld`、`WindowsAgentArena` 等基准上表现出色。\n\n**优势：**\n- **级联强化学习 (Cascade RL)：** 结合离线与在线RL，稳定高效地提升了模型的复杂推理能力。\n- **视觉分辨率路由器 (ViR)：** 动态调整图像压缩率，在不牺牲性能的前提下大幅降低了计算成本。\n- **解耦视觉语言部署 (DvD)：** 将视觉和语言模块部署在不同硬件上并异步处理，解决了计算瓶颈，显著加快了推理速度。","modelSize":"1B, 2B, 4B, 8B, 14B, 38B, 20B (激活4B), 30B (激活3B), 241B (激活28B)","modelType":"多模态","openSource":true,"contextWindow":"32K","officialDoc":"https://arxiv.org/abs/2408.14352"},{"date":"2025-08-22","title":"Mirage 2","text":"**摘要：** 全球首款AI原生UGC游戏引擎`Mirage`的2.0版本，是一个可在线游玩的实时通用领域生成式世界引擎。它能将任何图像（照片、绘画、涂鸦等）或文本提示，转化为可实时互动的3D世界，被视为对标谷歌`Genie 3`的重要产品。\n\n**模型规格与性能指标：**\n- **生成时长：** 可达10分钟以上。\n\n**性能与实测表现：**\n- **实测体验：** 当前版本体验不佳，存在明显的操控延迟、视角切换卡顿、画面帧率低以及视觉稳定性不足（如快速场景转换引入意外细节）等问题。\n\n**优势：**\n- **实时互动性强：** 用户可通过对话式文本提示，即时改变和重塑游戏世界。\n- **输入源多样：** 支持从照片到儿童涂鸦等多种形式的图像输入，并转化为可玩世界。\n- **低门槛运行：** 理论上可在消费级GPU上实现低延迟运行，潜力巨大。\n\n**劣势：**\n- **体验待优化：** 实际操控的延迟感和画面流畅度与现代游戏标准有较大差距。\n\n**适用场景 / 备注：**\n- 该模型展示了AI实时共同创作游戏的未来方向，让“世界模型”成为可在线游玩的产品。官方承认在动作控制精度和视觉稳定性方面仍存在前沿问题有待解决。","modelSize":"","modelType":"多模态, 视频生成, 世界模型","openSource":false,"contextWindow":"","officialDoc":"https://blog.dynamicslab.ai/"},{"date":"2025-08-21","title":"书生 Intern-S1-mini","text":"**摘要：** Intern-S1的80亿参数轻量化版本，旨在以极低的部署门槛提供强大的通用与专业科学能力，让更广泛的开发者和研究者能够轻松进行二次开发与应用。\n\n**模型规格与架构：**\n- **基础架构**：\n  - **语言模型**：基于8B参数的密集语言模型（`Qwen3`），提供强大的语言理解和生成能力。\n  - **视觉编码器**：结合0.3B参数的视觉编码器（`InternViT`），用在处理和理解图像数据。\n- **设计理念**：延续 `Intern-S1` 的设计理念，覆盖文本、图像、分子式、蛋白质等多模态、多任务数据领域，实现了轻量级模型下多种能力的极致平衡。\n\n**性能与实测表现：**\n- **通用能力**：在 `MMLU-Pro`、`AIME-2025`、`MMMU` 等多项权威基准上表现卓越，稳居同量级模型第一梯队。\n- **科学能力**：在化学 (`SmolInstruct`, `ChemBench`)、材料 (`MatBench`) 等任务中表现尤为突出，显著领先同级模型；在物理、地球、生物等学科中也保持领先水平。\n\n**优势：**\n- **轻量高效**：“减身材不减实力”，在参数规模与性能之间取得了良好平衡，大幅降低了对高端计算设备的依赖。\n- **易于开发**：上手门槛极低，仅需一张24GB显存的GPU即可完成LoRA微调，并已得到 `LLaMA-Factory` 等主流框架的支持。\n\n**适用场景 / 备注：**\n- 非常适合开展科学研究、在产品或应用中快速验证创意，以及在教学实践中让学生直观理解多模态大模型的原理与应用。","modelSize":"8B","modelType":"多模态, 语言","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/internlm/Intern-S1-mini"},{"date":"2025-08-20","title":"字节跳动 Seed-OSS","text":"**摘要：** 字节跳动Seed团队发布的360亿参数开源大语言模型，以原生的512K超长上下文窗口和创新的“思考预算”控制机制为主要看点，在多个英文基准测试上刷新了SOTA记录。\n\n**模型规格与架构：**\n- **参数量：** `36B`（稠密模型）\n- **架构：** Causal LM，采用`RoPE`位置编码、`GQA`注意力机制、`RMSNorm`归一化和`SwiGLU`激活函数。\n- **上下文：** 原生支持 `512K`。\n- **训练数据：** 12T Tokens，知识截止时间为2024年7月。\n- **版本：** 提供 `Seed-OSS-36B-Base`（含合成数据）、`Seed-OSS-36B-Base-woSyn`（不含合成数据）和 `Seed-OSS-36B-Instruct`（指令微调）三个版本。\n\n**性能与实测表现：**\n- **基准：** 在 `BBH` 推理基准上刷新了开源模型纪录。在 `MMLU-Pro`、`GSM8K`、`MATH`、`HumanEval` 等多个知识、数学、代码类基准上均表现出色，性能超越了Qwen2.5-32B等同级别模型。\n- **实测：** 指令微调版在 `AIME24` 数学竞赛题上表现优异。模型在工具调用（Agent）任务上经过特别优化，表现突出。\n\n**优势：**\n- **超长上下文：** 原生512K上下文长度，是发布时主流开源模型的4倍，无需插值扩展，适合处理海量信息场景。\n- **思考预算（Thinking Budget）：** 允许用户灵活设定token预算来控制模型的思考深度，以平衡性能与推理成本，实用性强。\n- **研究友好：** 提供了不含合成数据的“纯净”预训练版本，方便学术界进行后续研究。\n- **性能优异：** 以相对较少（12T）的训练数据量，在多个领域达到了SOTA性能。\n\n**劣势：**\n- **语言偏向性：** 模型主要针对英语进行了优化和评估，在中文等其他语言上的性能有限且未经严格测试，对于非英语场景应用价值受限。\n\n**适用场景 / 备注：**\n- 适用于法律文档审查、长篇报告分析、复杂代码库理解等需要处理海量信息的专业场景。\n- 使用 `vLLM` 框架进行推理时，需要安装指定版本。","modelSize":"36B","modelType":"语言","openSource":true,"contextWindow":"512K","officialDoc":"https://huggingface.co/collections/ByteDance-Seed/seed-oss-68a609f4201e788db05b5dcd"},{"date":"2025-08-20","title":"⭐ DeepSeek V3.1","text":"**摘要：** DeepSeek V3.1 是一款备受瞩目的混合推理（Hybrid Reasoning）MoE模型，以极高的性价比在编程和推理任务上展现出强大实力。然而，社区评测显示其表现“偏科”，在取得优异基准成绩的同时，也在物理模拟、视觉创意等特定领域暴露明显短板。其采用的 `UE8M0 FP8` 精度格式，被视为推动国产AI芯片软硬协同的关键一步，引发了市场广泛关注。\n\n**模型规格与架构：**\n- **参数量：** 采用专家混合（MoE）架构，总参数量6710亿，激活参数量370亿。\n- **架构/范式：** 混合推理模型，整合了“思考”（类似 `R1`）与“非思考”（类似 `V3`）两种模式。新增了用于原生搜索的 `<｜search begin｜>` 和显式思维链的 `<think>` 等特殊Token。\n- **上下文：** 128K。\n- **精度格式：** 采用了 `UE8M0 FP8 Scale` 参数精度，该格式能大幅节省内存带宽，旨在深度适配和优化国产AI芯片的性能。\n\n**性能与实测表现：**\n- **基准：** 在 `Aider` 编程基准上得分71.6%，超越了 `Claude 3 Opus`，且成本仅为其1/68。\n- **实测（综合社区评测）：**\n  - **优势领域：** 在常规编程（如网页开发）、逻辑推理（表现不输于 `Gemini 2.5 Pro`）和工具调用（稳定性提升）方面表现出色。`非思考`模式在小说开篇等任务上甚至优于`思考`模式。\n  - **劣势领域：** 表现两极分化。在需要精确物理模拟（如p5.js小球弹跳）或特定审美（如网页设计）的编程任务中惨败。\n  - **稳定性：** 长文本生成时，网页端出现过突然中断、内容清空并“失忆”的Bug。`思考`模式在小说续写中也出现了前后矛盾。\n\n**优势：**\n- **极高性价比：** 在其擅长的领域，提供了顶级的性能和极低的成本，对开发者极具吸引力。\n- **软硬协同：** 率先采用 `UE8M0 FP8` 格式，主动适配国产芯片硬件，为国产AI生态建设提供了范例。\n\n**劣势：**\n- **能力偏科严重：** 模型能力不均衡，是一个“偏科生”，在某些领域表现极佳，在另一些领域则完全不可用。\n- **审美与物理理解欠缺：** 代码能力似乎停留在“知其然，不知其所以然”的层面，缺乏对物理规律和视觉美学的深入理解。\n- **长上下文稳定性存疑：** 在长文本任务中暴露出稳定性问题，可能与Web端审查或缓存机制有关。","modelSize":"671B (激活 37B)","modelType":"语言","openSource":true,"contextWindow":"128K","officialDoc":"https://huggingface.co/deepseek-ai/DeepSeek-V3.1-Base","evaluation":"预期过高，但股市又涨了一波。"},{"date":"2025-08-19","title":"Alibaba Ovis 2.5","text":"**摘要：** 阿里巴巴发布的开源多模态大语言模型（MLLM），专为原生分辨率视觉感知和深度多模态推理而设计。其核心特点是采用了原生分辨率视觉变换器（NaViT）和可选的“思考模式”，在多个基准测试中达到了同规模模型的SOTA水平。\n\n**模型规格与架构：**\n- **参数量：** `9B`、`2B`\n- **架构/范式：** 沿用`Ovis`架构（视觉分词器VT、视觉嵌入表VET、LLM），但进行了两大升级：\n  - **视觉编码器：** 升级为原生分辨率的`NaViT`（基于`siglip2-so400m-patch16-512`初始化），可处理任意分辨率和宽高比的图像，无需裁剪或切片。\n  - **语言模型基座：** 升级为`Qwen3`，以增强深度推理能力。\n\n**性能与实测表现：**\n- **综合基准：** 在OpenCompass多模态排行榜上，`Ovis2.5-9B`平均分达78.3，在40B参数以下的开源MLLM中排名第一；`Ovis2.5-2B`得分为73.9，在同级别中表现优异。\n- **专项能力：** 在STEM推理（`MMMU`）、复杂图表分析（`ChartQA Pro`）、OCR（`OCRBench`）、视觉定位（`RefCOCO`）以及视频理解等多个领域均展示了顶尖或领先的性能。\n\n**优势：**\n- **原生分辨率感知：** 无需破坏性的图像切片，能最大限度保留图像的精细细节和全局结构，特别适合处理文档和图表。\n- **深度推理能力：** 可选的“思考模式”引入了反思和自我修正机制，允许模型在处理复杂问题时进行更深入的思考，以换取更高的准确性。\n- **高效训练：** 通过数据打包和混合并行框架等技术优化，实现了3至4倍的端到端训练速度提升。\n\n**适用场景/备注：**\n- 适用于对视觉细节要求高的场景，如复杂图表分析、科学文献理解、文档/表单OCR等。同时在需要严密逻辑推理的视觉问答任务中表现突出。\n\n**与Qwen-VL系列对比：**\n- **定位差异：** `Ovis 2.5`更像是“技术探索先锋”，专注于攻克原生分辨率处理、深度反思推理等前沿难题；`Qwen-VL`系列则定位为“通用多模态基石”，追求在广泛任务上的均衡与极致性能。\n- **视觉处理：** `Ovis 2.5`采用`NaViT`，可直接处理任意分辨率的原始图像，无损细节；`Qwen-VL`处理高清图则通常采用“切片（tiling）”策略。\n- **推理机制：** `Ovis 2.5`引入了独特的“思考模式”，进行显式的自我检查与修正；`Qwen-VL`具备强大的通用推理能力，但没有此种特定的反思机制。\n- **关系：** `Ovis`系列的技术探索成果未来可能会融入`Qwen`主系列，可以理解为“专才”与“全才”的关系。","modelSize":"9B, 2B","modelType":"多模态, 文档解析模型","openSource":true,"contextWindow":"","officialDoc":"https://arxiv.org/abs/2508.11737"},{"date":"2025-08-15","title":"腾讯 HunyuanWorld-1.0-lite","text":"**摘要：** 腾讯于8月15日正式发布 `HunyuanWorld-1.0` 的量化版本 `HunyuanWorld-1.0-lite`。该版本是继7月26日初始模型发布后的重大更新，通过量化技术显著降低了硬件门槛，首次实现了在 `4090` 等消费级GPU上运行，极大地提升了模型的可及性和实用性。\n\n![](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/raw/main/assets/teaser.png)\n\n**模型规格与架构：**\n- **核心架构：** 沿用 `HunyuanWorld 1.0` 的核心框架，即“全景代理生成 → 语义分层 → 层次化3D重建”。\n![](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/raw/main/assets/arch.jpg)\n- **Lite版关键优化：** 为突破显存瓶颈，Lite版本引入了关键的量化与缓存技术，用户可通过添加 `--fp8_gemm`、`--fp8_attention` 和 `--cache` 等参数来启用。\n\n**性能与实测表现：**\n- **硬件兼容性：** 成功将模型部署在消费级显卡上，解决了初始版本对高端计算资源（>26GB显存）的依赖。\n- **生成质量：** 在大幅降低资源消耗的同时，保持了与原版几乎无异的高质量3D世界生成效果。根据官方报告，模型在多项视觉质量和几何一致性指标上（如BRISQUE, NIQE, Q-Align, CLIP-T/I）均优于其他开源方法。\n- **视觉效果：**\n![](https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0/raw/main/assets/roaming_world.gif)\n\n**优势：**\n- **极高的可及性：** Lite版本的发布是该项目的里程碑，使广大个人开发者和小型团队也能轻松体验和使用前沿的3D世界生成技术。\n- **完整的开源生态：** 提供从推理代码、模型权重到技术报告的全套资源，并配套了模型浏览器等工具。\n- **强大的兼容性：** 生成的 `3D mesh` 格式可无缝接入游戏开发、物理仿真、VR/AR等现有计算机图形（CG）工作流。\n\n**适用场景：**\n![](https://github.com/Tencent-Hunyuan/Hunyu_anWorld-1.0/raw/main/assets/application.png)\n- 游戏开发\n- 虚拟现实与增强现实\n- 影视制作与广告\n- 交互式内容创作","modelSize":"PanoDiT-Text: 478MB, PanoDiT-Image: 478MB, PanoInpaint-Scene: 478MB, PanoInpaint-Sky: 120MB","modelType":"多模态, 3D世界生成","openSource":true,"contextWindow":"","officialDoc":"https://github.com/Tencent-Hunyuan/HunyuanWorld-1.0","evaluation":"Lite版本的发布是HunyuanWorld 1.0项目走向普及应用的关键一步。它不仅在技术上展示了SOTA的性能，更重要的是通过工程优化解决了实际部署中的核心痛点，使其成为目前对个人开发者最友好的开源3D世界生成模型之一。"},{"date":"2025-08-11","title":"⭐ 智谱 GLM-4.5V","text":"**摘要：** 智谱AI发布的旗舰级开源多模态模型，基于`GLM-4.5-Air`构建。它在42个公开基准中的41个上取得SOTA性能，并以其强大的原生视频理解、视觉定位（Grounding）和网页复刻能力在社区实测中获得高度评价。\n\n**模型规格与架构：**\n- **参数量**：106B总参数，12B激活参数。\n- **架构/范式**：采用`MoE`（专家混合）架构，延续`GLM-4.1V-Thinking`技术路线，文本基座为`GLM-4.5-Air-Base`。\n- **其他特性**：新增“思考模式”开关，允许用户在追求速度和深度推理之间进行灵活切换。\n\n**性能与实测表现：**\n- **基准**：官方宣称在42个公开多模态榜单中，41个达到SOTA，综合性能领先同级别开源模型。\n- **实测**：在社区测试中，于专业读数（如游标卡尺）、复杂场景推理、网页复刻、原生视频内容分析、视觉定位等高级任务上表现出色且快速。\n\n**优势：**\n- **原生视频理解**：具备真正的时空理解能力，能按时间戳分析视频画面内容，而非简单的音频转录。\n- **强大的文档与代码能力**：在OCR、表格识别（可还原为HTML）、网页复刻等任务上表现顶尖。\n- **精准的视觉定位（Grounding）**：能准确识别并框选图像中的指定元素，在特定任务中切换到Grounding模式可显著提升准确率。\n\n**劣势：**\n- **高阶空间逻辑推理**：与其他多模态模型类似，在复杂的空间变换和逻辑推理任务上存在明显短板。\n- **细粒度识别**：在区分度极低的场景（如高度相似的建筑内景）中可能出错。\n- **部署门槛**：模型体量较大（106B），对本地部署有较高硬件要求。\n\n\n```json\n## Role\n你是一位有多年经验的OCR表格识别专家。\n## Goals\n需要通过给定的图片，识别表格里的内容，并以html表格结果格式输出结果。\n## Constrains\n- 需要认识识别图片中的内容，将每个表格单元格中的内容完整的识别出来，并填入html表格结构中；\n- 图片中的表格单元格中可能存在一些占位符需要识别出来，例如\"-\"、\"—\"、\"/\"等；\n- 输出表格结构一定遵循图片中的结构，表格结构完全一致；\n- 特别注意图片中存在合并单元格的情况，结构不要出错；\n- 对于内容较多的图片，一定要输出完整的结果，不要断章取义，更不要随意编造；\n- 图片内容需要完整识别，不要遗漏，同时注意合并单元；\n- 最终输出结果需要是html格式的表格内容。\n## Initialization\n请仔细思考后，输出html表格结果。\n```\n测试双栏检验单，慢思考模型下真的很不错。\n\n**其他 - Grounding**\n**普通问法（可能得到文本回答）：**\n`\"图片中奔跑的人在第几行第几列？\"`\n`\"图里有几串烤串？\"`\n**Grounding问法（触发图片标记）：**\n`\"请**框选出**图片中奔跑的人。\"`\n`\"请帮我**圈出**图里所有的烧烤签子。\"`\n`\"**标记出**图里的郭帆导演。`","modelSize":"106B (总计 106B, 激活 12B)","modelType":"多模态、文档解析模型、代码","openSource":true,"contextWindow":"","officialDoc":"https://github.com/zai-org/GLM-V","evaluation":"曾经的国产之光，智谱好像回来了。\n赞美开源。"},{"date":"2025-08-11","title":"Baichuan Baichuan-M2-32B","text":"**摘要：** 百川智能发布的32B开源医疗增强推理模型，在权威医疗评测`HealthBench`上超越OpenAI的`gpt-oss-120b`登顶SOTA。该模型基于创新的“大型验证器系统”和“患者模拟器”，在保持通用能力的同时，大幅提升了医疗推理能力，并支持RTX 4090单卡部署，部署成本极低。\n\n**模型规格与架构：**\n- **参数量：** `32B`\n- **基础模型：** 基于 `Qwen/Qwen2.5-32B`\n- **核心架构/范式：**\n  - **大型验证器系统 (Large Verifier System)：** 结合医疗场景特点设计，包含“患者模拟器”和多维度验证机制，为强化学习提供高质量、动态的奖励信号。\n  - **医疗领域自适应增强：** 采用中期训练（Mid-Training）方式高效注入医疗知识，同时保持通用能力。\n  - **多阶段强化学习 (Multi-Stage RL)：** 采用改进的`GRPO`算法，分阶段培养模型的医学知识、推理和医患交互能力。\n- **上下文：** `32K`\n\n**性能与实测表现：**\n- **医疗基准 (`HealthBench`)：**\n  - 标准版得分 `60.1`，超越 `gpt-oss-120b`（57.6分）和 `Qwen3-235B`（55.2分）。\n  - 困难版 (`HealthBench-Hard`) 得分 `34.7`，远超 `gpt-oss-120b`（30分），与 `GPT-5` 是唯二超过32分的模型。\n- **通用能力基准：** 在`AIME24/25`、`Arena-Hard-v2.0`、`WritingBench`等通用评测上，全面超越同量级的`Qwen3-32B`。\n- **本土化实测：** 在中国临床诊疗场景（如肝癌治疗案例）中，其推荐方案比`gpt-oss`系列更贴合国内权威指南，本土化优势明显。\n\n**优势：**\n- **性能卓越：** 以更小模型尺寸在医疗评测上实现SOTA，尤其在复杂、困难场景下表现突出。\n- **成本极低：** 经过4-bit量化后支持单张消费级显卡（RTX 4090）部署，极大降低了私有化门槛。\n- **技术创新：** 首创“患者模拟器”和大型验证器系统，为解决医疗领域AI训练难题提供了新范式。\n- **能力均衡：** 在强化医疗能力的同时，通用核心能力不降反增。\n\n**适用场景 / 备注：**\n- 适用于临床辅助诊断、智能问诊、医学知识问答与教育、健康咨询等场景。\n- 官方强调，模型仅供研究和参考，不能替代专业的医疗诊断或治疗。","modelSize":"32B","modelType":"语言, 医疗","openSource":true,"contextWindow":"32K","officialDoc":"https://www.baichuan-ai.com/blog/baichuan-M2","evaluation":"32B太香了，但具体效果如何，还得在实际业务中见真章。"},{"date":"2025-08-08","title":"⭐OpenAI GPT-5","text":"OpenAI于2025年8月8日正式发布其下一代旗舰模型GPT-5。该模型被定义为一个统一的集成系统，而非单一模型。\n\n**核心架构与模型系列：**\nGPT-5系统整合并重命名了多个先前的模型，形成了一个统一的家族。其核心是“快思考”的`main`系列和“慢思考”的`thinking`系列，由一个智能路由器根据任务需求自动调用。详细的模型继承关系如下：\n* `GPT-4o` -> `gpt-5-main`\n* `GPT-4o-mini` -> `gpt-5-main-mini`\n* `OpenAI o3` -> `gpt-5-thinking`\n* `OpenAI o4-mini` -> `gpt-5-thinking-mini`\n* `GPT-4.1-nano` -> `gpt-5-thinking-nano`\n* `OpenAI o3 Pro` -> `gpt-5-thinking-pro`\n\n此对应关系明确显示，`main`系列是GPT-4o的直接延续，而`thinking`系列则整合了之前独立的、推理能力更强的o3、o4等实验性模型分支。\n\n**性能与实测表现：**\n* **基准测试**: 在数学、编程、多模态等多个基准测试中全面领先，成为大模型盲测竞技场的新榜首。同时，模型效率也得到提升，在完成复杂任务时输出的Token减少了50-80%。\n* **减少幻觉与谄媚**: 大幅降低了事实性错误的发生率（`gpt-5-thinking`比o3少五倍以上），并减少了不必要的迎合，对话体验更像与真人交流。\n* **编程能力 (高度好评)**: 被评为“一骑绝尘”，尤其在前端UI/UX设计、生产级代码的精准修改和高上下文精度方面表现惊人。实测中能从一句话生成复杂的可视化效果、可玩游戏，并完成了其他顶级模型失败的编码任务。\n* **写作能力 (评价分裂)**: 受到部分用户，特别是重度写作者的批评，认为其文笔相较于已被移除的GPT-4.5有明显退步，风格变得蹩脚，指令遵循能力一般。\n\n**定价与API：**\n* **价格**: API定价极具竞争力，输入为$1.25/百万token，输出为$10/百万token，低于主要竞争对手。\n* **API升级**: API迎来了多项对开发者友好的新特性。","modelSize":"","modelType":"多模态","openSource":false,"contextWindow":"400K","officialDoc":"https://openai.com/gpt-5/","evaluation":""},{"date":"2025-08-07","title":"MinerU2","text":"上海人工智能实验室发布了全新架构的智能文档解析引擎MinerU2，基于 **“通专融合”** 技术路线，旨在将复杂文档（特别是科学文献）精准转化为高质量的“AI-Ready”数据。\n\n### 模型规格与性能\n* **模型**: 端到端多模态文档解析大模型\n* **参数量**: **0.9B**\n* **性能**: 解析准确率较前代提升 **22%**，速度提升 **6倍**。性能比肩72B大模型，可在消费级显卡上运行。在同级别开源模型综合评分中位列第一。\n* **核心能力**: 除了高精度解析文本、布局、表格、公式外，新增了对化学分子式和化学反应的解析能力。\n\n### 主要功能更新 (客户端/网页端)\n* **化学解析 (MinerU.Chem)**: 新增化学论文解析能力，可提取化合物结构和化学反应，并支持交互式预览和导出。\n* **多模式翻译**: 上线全文和划词翻译功能，支持GPT-4o-mini、DeepL、Google等多种翻译引擎。\n* **知识管理**: 新增收藏、批注、反馈功能，提升用户交互和知识沉淀效率。\n* **工作流集成**: 支持一键导出到Notion和Dify，并可为超长文档设置按页码范围解析。\n\n### 生态与部署\n* 项目已在GitHub开源，并提供了桌面客户端、网页端、API及企业级服务等多种使用形态。","modelSize":"0.9B","modelType":"文档解析模型","openSource":true,"contextWindow":"","officialDoc":"https://github.com/opendatalab/MinerU","evaluation":"实测下来不及预期，在医疗检验单场景下测试，发现以下主要问题：\n - **版面理解能力不足**：出现将文本标题错误识别为图片的情况。\n - **表格解析准确率低**：表格内的数据提取错误率偏高。"},{"date":"2025-08-07","title":"OpenAI GPT-OSS","text":"OpenAI于2025年8月初发布了其自GPT-2以来的首个“开放权重”(open-weight)模型系列——GPT-OSS，包含gpt-oss-120b和gpt-oss-20b两个模型，采用Apache 2.0许可。这意味着模型权重公开，但训练代码和数据集不公开。\n\n**模型规格与架构:**\n- **gpt-oss-120b**: 拥有1170亿总参数，采用混合专家（MoE）架构，每个token激活51亿参数。官方称其性能接近闭源的o4-mini，可在单张80GB GPU上运行。\n- **gpt-oss-20b**: 拥有210亿总参数，激活36亿参数。性能接近o3-mini，可在16GB内存的消费级设备（笔记本、手机）上运行。\n两个模型均采用Transformer架构，支持128k上下文窗口，使用了类似GPT-3的交替稀疏注意力模式、分组多查询注意力（GQA），并开源了其o200k_harmony分词器。后训练流程（SFT、RL）与o4-mini相似，并支持高、中、低三种推理级别。\n\n**性能表现:**\n官方基准测试显示，GPT-OSS在工具调用（搜索、代码执行）、思维链推理和特定基准（如HealthBench）上表现强劲，部分指标甚至超越了GPT-4o和o1。\n然而，多方独立实测揭示了其优势与短板：\n- **优势**: 在结构化的数学和逻辑推理任务上表现非常出色，能够清晰地解决复杂的数学题。\n- **劣势**: 在更泛化的任务上表现不佳。尤其在代码生成方面，无法完成稍复杂的项目，表现“一言难尽”，远不如Qwen3等竞品；在需要整合多条信息的复杂推理题（如七个小矮人）上会出错或卡死；在创意性写作和角色扮演方面也同样逊色。","modelSize":"120B (总计 117B, 激活 5.1B), 20B (总计 21B,激活 3.6B)","modelType":"语言大模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4","evaluation":"官方数据惊艳，实测表现拉垮。擅长数学推理，但在编程和复杂通用任务上远不及预期，综合能力不如同级别的国产开源模型。"},{"date":"2024-08-06","title":"小红书 dots.vlm1","text":"**摘要：** 小红书发布的首个开源多模态大模型，核心是自研的1.2B `NaViT`视觉编码器和`DeepSeek V3`语言模型，在处理图表、文档等结构化图像方面表现出色。\n\n**模型规格与架构：**\n- **核心组件：**\n  - **视觉编码器：** 自研 `NaViT` (12亿参数)，从零开始训练，支持动态分辨率。\n  - **语言模型：** `DeepSeek V3 MoE`\n  - **连接器：** `MLP` 适配器\n- **上下文窗口：** `4K` (4096 tokens)\n\n**性能与实测表现：**\n- **基准评测：** 在 `MMMU`、`MathVision`、`OCR Reasoning` 等多个国际权威多模态评测集上表现优异，整体性能接近 `Gemini 1.5 Pro` 和 `Seed-VL 1.5` 等顶尖闭源模型。\n- **核心能力：** 特别擅长处理复杂的图表推理、STEM数学问题和长尾特定场景识别。\n\n**优势：**\n- **创新视觉编码器：** `NaViT` 编码器专为结构化图像优化，提升了模型对表格、图表、公式的理解能力。\n- **强大的推理能力：** 结合 `DeepSeek V3`，在多模态综合推理任务上展现出强大实力。\n- **完全开源：** 采用 `MIT` 许可证，对学术界和产业界友好，可商用。\n\n**适用场景：**\n- 通用视觉问答与对话\n- 复杂图表与文档内容理解\n- OCR与版面分析\n- STEM领域的多模态辅助","modelSize":"视觉编码器 1.2B","modelType":"多模态","openSource":true,"contextWindow":"4K","officialDoc":"https://github.com/rednote-hilab/dots.vlm1"},{"date":"2025-08-05","title":"Qwen-Image","text":"阿里千问团队发布的Qwen-Image是其首个图像生成基础模型，基于20B参数的MMDiT架构。该模型在文生图方面表现出色，尤其擅长中英文文本的精准渲染，能将文字与图像无缝融合。它支持通用图像编辑（如风格迁移、物体增删、姿态调整）、多种艺术风格生成，并能处理物体检测、语义分割等计算机视觉任务。Qwen-Image在多个公开基准测试中取得SOTA，是当前Artificial Analysis Image Arena上排名最高的开源模型。\n\n**MMDiT**，全称为**多模态扩散变换器（Multimodal Diffusion Transformer）**，是当前AI图像生成领域一项前沿的核心技术架构。它是一种专门设计用来处理和融合多种不同类型数据（即“模态”），如文本描述和图像信息的神经网络模型。\n\n这项技术是继“扩散模型（Diffusion Model）”和“视觉变换器（Vision Transformer, ViT）”之后的又一重要演进，通过其独特的结构，显著提升了AI模型根据复杂文字提示生成高质量、高相关性图像的能力。\n\n**核心思想：为不同信息类型设立“专属通道”**\n在传统的图像生成模型中，文本指令通常通过一个称为“交叉注意力（Cross-Attention）”的机制被注入到图像生成过程中。你可以把它想象成，在处理图像的同时，模型会时不时地“瞥一眼”文本指令来获取引导。\n\nMMDiT则采用了更为精妙的方法。它的核心创新在于：\n\n1. **分离处理**：MMDiT为文本信息和图像信息设立了两套独立的处理模块（或使用独立的权重）。这意味着文本和图像的特征（Tokens）在各自的“专属通道”中被分别处理，保留了各自模态的独特性和完整性。\n\n2. **联合注意力**：在经过初步的独立处理后，两条通道中的信息会被**拼接（concatenate）**在一起，送入一个统一的注意力模块中进行联合处理。这使得文本和图像信息能够在更深层次上进行交互和对齐，模型可以更精确地理解“宇航员”、“骑着猪”、“穿着芭蕾舞裙”等不同概念应该如何组合在同一画面中。\n\n简单来说，MMDiT架构就像一个拥有两个独立专业团队（一个文本专家团队，一个图像专家团队）的工作室。两个团队先分别消化自己的任务信息，然后在关键节点上进行高效的协同工作，共同完成最终的创作。","modelSize":"20B (MMDiT)","modelType":"图像生成、图像编辑","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/Qwen/Qwen-Image","evaluation":"左手“万相”拍视频，右手“千问”P图片，阿里这是要承包我的创意文件夹吗？"},{"date":"2025-08-04","title":"小米 MiDashengLM-7B","text":"**摘要：** 小米发布并开源的70亿参数多模态声音理解大模型，在速度与精度上取得双重突破，刷新了22个公开评测集SOTA，旨在赋能其“人车家全生态”战略。\n\n**模型规格与架构：**\n- **参数量：** 7B\n- **架构/范式：** 采用双核设计，结合了小米自研的开源音频编码器`Xiaomi Dasheng`和`Qwen2.5-Omni-7B Thinker`作为自回归解码器。\n- **训练方法：** 采用创新的“通用音频描述”（general audio caption）训练策略，能够统一处理和理解语音、环境音、音乐等多种音频信号。\n\n**性能与实测表现：**\n- **基准：** 在22个公开评测集上刷新了多模态大模型的最佳成绩（SOTA）。\n- **效率：** 推理效率极高，单样本首Token延迟（TTFT）仅为同类先进模型的1/4，数据吞吐效率提升20倍以上。\n\n**适用场景 / 备注：**\n- 核心应用场景为小米的“人车家全生态”，如智能座舱、智能家居等。","modelSize":"7B","modelType":"多模态","openSource":true,"contextWindow":"","officialDoc":"https://github.com/xiaomi-research/dasheng-lm"},{"date":"2025-08-04","title":"Hunyuan 腾讯混元小尺寸模型系列","text":"### 核心发布：端侧开源小尺寸语言模型\n腾讯于2025年8月4日宣布，正式开源四款专为**端侧设备**设计的小尺寸语言模型，旨在推动AI在消费电子、智能汽车等领域的应用。\n同时​增强了Agent能力，在BFCL-v3、τ-Bench、C3-Bench等智能体基准测试中领先\n\n### 模型规格\n该系列包含四个不同规模的模型，以满足不同场景的需求：\n* **0.5B (5亿参数)**\n* **1.8B (18亿参数)**\n* **4B (40亿参数)**\n* **7B (70亿参数)**\n\n### 主要优势\n* **高效运行**: 能够在笔记本、手机、智能座舱等低功耗设备上高效部署。\n* **易于微调**: 支持在垂直领域进行低成本的定制化微调。\n\n### 其他近期开源\n此次发布是腾讯近期密集开源动作的一部分，此前还开源了**混元3D世界模型1.0**（2025年7月）和**混元Hunyuan-A13B** MoE模型（2025年6月）。","modelSize":"0.5B, 1.8B, 4B, 7B","modelType":"语言大模型（快思考和慢思考）","openSource":true,"contextWindow":"256K","officialDoc":"https://huggingface.co/tencent-hunyuan","evaluation":"小小杯、超小杯、超超小杯"},{"date":"2025-08-01","title":"阶跃Step-3","text":"#### \n阶跃星辰最新开源的Step-3是一款SOTA级基础模型，其核心设计理念是 **“模型-Infra协同设计”** ，旨在实现极致的推理效率和性价比。它在MMMU等多个多模态榜单上取得了开源模型的新SOTA成绩。\n\n### 模型规格\n* **架构**: MoE (混合专家) 架构，包含48个专家\n* **参数量**: 总参数量 321B (316B语言 + 5B视觉)，激活参数量 38B\n* **性能**: 在Hopper GPU上解码速度高达 **4039 token/秒**，是DeepSeek-V3的174%（4K上下文、FP8、无MTP）。\n* **成本**: 性价比极高，在特定硬件组合下，**百万Token解码成本不到4毛钱人民币**，在H20计算卡上解码成本仅为DeepSeek-V3的30%。\n\n### 技术亮点\n* **模型-Infra协同设计**: 将模型架构、系统调度和硬件Infra作为一个整体进行优化。\n* **MFA注意力机制**: 采用自研的MFA（多矩阵因子分解）注意力机制，从根源上压缩KV缓存，降低计算消耗，尤其适合长上下文场景。\n* **AFD解耦系统**: 提出AFD（Attention-FFN Disaggregation）机制，将Attention和FFN计算任务拆分到各自最适合的GPU集群，通过自研的**StepMesh通信库**连接，实现整体吞吐效率最大化。\n* **开源开放**: 模型及核心的StepMesh通信库均已开源。\n\n### 部署能力\n* 可在8块48GB的GPU上运行，并处理高达80万个Token的上下文。","modelSize":"321B (激活 38B)","modelType":"多模态大模型","openSource":true,"contextWindow":"64K","officialDoc":"https://github.com/stepfun-ai/Step3"},{"date":"2025-07-30","title":"⭐dots.ocr","text":"dots.ocr是由小红书发布的、基于17亿参数大语言模型的多语言文档解析模型。它将版面检测与内容识别统一在单一的视觉语言模型中，在OmniDocBench等多个基准测试中，其文本、表格及阅读顺序解析能力均达到SOTA（业界最佳）水平，并对低资源语言有出色的支持。","modelSize":"1.7B","modelType":"文档多模态模型/文档解析","openSource":true,"contextWindow":"","officialDoc":"https://github.com/rednote-hilab/dots.ocr","evaluation":"1.7B小钢炮，实测效果好于Mineru。开源SOTA级别，官方提供的评测结果中，遥遥领先于其他多款模型或工具。"},{"date":"2025-07-30","title":"Qwen3-30B-A3B-Instruct-2507","text":"\n- 显著提升了包括 指令执行、逻辑推理、文本理解、数学、科学、编码和工具使用 在内的通用能力。\n- 大幅增加 了 多种语言 中的长尾知识覆盖范围。\n- 显著更好地符合 用户在 主观和开放式任务 中的偏好，能够提供更有帮助的响应和更高质量的文本生成。","modelSize":"30.5B(激活 3.3B)","modelType":"","openSource":true,"contextWindow":"256K","officialDoc":""},{"date":"2025-07-30","title":"Qwen3-30B-A3B-Thinking-2507","text":"- 在逻辑推理、数学、科学、编码和通常需要人类专业知识的学术基准等推理任务上显著提升的性能。\n - 明显更好的通用能力，如指令执行、工具使用、文本生成以及与人类偏好的一致性。\n - 增强的256K长上下文理解能力。\n此版本的思考长度有所增加。建议在高度复杂的推理任务中使用它。","modelSize":"30.5B(3.3B)","modelType":"","openSource":true,"contextWindow":"256K","officialDoc":""},{"date":"2025-07-28","title":"⭐(智谱AI) GLM-4.5","text":"智谱（Zhipu）发布的GLM-4.5是一款强大的多模态模型，拥有3550亿总参数和320亿激活参数。该模型在复杂推理和代码生成方面表现出色，是面向智能体应用的下一代基础模型。","modelSize":"355B(激活 32B), 106B(激活 12B)。GLM-4.5: 总参数355B，激活参数32B；GLM-4.5-Air: 总参数106B，激活参数12B","modelType":"多模态大模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/ZhipuAI/GLM-4.5-32B-Code-Instruct"},{"date":"2025-07-28","title":"通义万相2.2 (Tongyi Wanxiang 2.2)","text":"通义万相2.2是阿里巴巴于2025年7月28日发布的、主打**视频生成**的新一代AI模型，其目标是实现“电影级”的创作水准。\n\n### 主要功能\n* **文生视频 (Text-to-Video)**：根据文本描述生成高清视频。\n* **图生视频 (Image-to-Video)**：将静态图片转化为内容和风格一致的动态视频。\n* **统一视频模型**：发布了一款5B参数的统一模型，同时支持文生视频和图生视频，降低了部署门槛。\n* **文生图**：系列中也包含了升级的文生图模型。\n\n### 技术亮点\n* **首创MoE架构**: 业界首个在视频生成中采用混合专家（MoE）架构的模型，有效平衡了效果与计算效率。\n* **电影美学控制系统**: 内置超过60个美学参数，允许用户精细控制光影、色彩、镜头语言等视觉风格。\n* **强大的动态表现**: 在处理复杂运动、保持角色一致性和还原微表情等细节方面表现出色。","modelSize":"27B (MoE, 激活14B), 5B","modelType":"视频生成","openSource":true,"contextWindow":"","officialDoc":"https://modelscope.cn/organization/iic"},{"date":"2025-07-27","title":"Qwen3-Coder-Plus","text":"阿里巴巴（Alibaba）发布的Qwen3-Coder-Plus是Qwen3系列中专为代码生成和补全而优化的模型。它在多种编程语言和框架上都表现出色，能够显著提升开发效率。","modelSize":"","modelType":"文本 (代码)","openSource":true,"contextWindow":"","officialDoc":"https://huggingface.co/Qwen/Qwen3-Coder-72B-G-Instruct"},{"date":"2025-07-26","title":"书生 Intern-S1","text":"**摘要：** 上海人工智能实验室发布的全球领先开源科学多模态大模型，被誉为“全能高手+科学明星”，旨在通过深度融合通用与顶尖科学能力，重构科研生产力。\n\n**模型规格与架构：**\n- 参数量：28B激活参数（总计241B）。\n- 架构/范式：基于 `Qwen3-235B` 和 `InternViT-6B` 构建的混合专家（MoE）架构。\n- 核心创新：首创“跨模态科学解析引擎”，并采用动态Tokenizer和时序信号编码器，原生支持化学分子式、蛋白质序列、地震波等多种复杂科学模态数据的深度融合。\n\n**性能与实测表现：**\n- 多模态通用能力：综合能力在开源模型中排名第一，得分比肩国内外一流闭源模型。\n- 科学专业能力：在化学、材料、地球科学等多个专业任务基准上，性能超越了 `Grok-4` 等顶尖闭源模型，树立了行业新标杆。\n- 训练优化：通过系统与算法的联合优化，实现了大型多模态MoE模型在FP8精度下的高效强化学习，成本降低10倍。\n\n**优势：**\n- **顶尖科学能力**：专为前沿科学研究设计，在处理和理解高度专业化的科学数据方面具有显著优势。\n- **开源与生态**：模型及全链路工具链（微调、部署、评测）全面开源，支持免费商用，社区生态活跃。\n\n**适用场景 / 备注：**\n- 旨在成为“科研搭档”而非简单的“对话助手”，适用于化合物合成路径预测、化学反应可行性判断、生物信息学分析等前沿科研任务。\n- 其轻量级版本 `Intern-S1-mini` 于2025年8月21日发布。","modelSize":"241B (激活 28B)","modelType":"多模态, 语言, 科学计算","openSource":true,"contextWindow":"","officialDoc":"https://github.com/InternLM/Intern-S1"},{"date":"2025-07-25","title":"阿里巴巴 Qwen3-235B-A22B-Thinking-2507","text":"Qwen3-Thinking 是阿里巴巴Qwen团队推出的 MoE 模型，专注于复杂认知任务、多步推理和抽象思维，旨在处理高级逻辑和分析任务。","modelSize":"235B(激活 22B)","modelType":"语言模型 (MoE，推理强化)","openSource":true,"contextWindow":"128K","officialDoc":"https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507"},{"date":"2025-07-25","title":"阿里巴巴 Qwen3-Coder-480B-A35B-Instruct","text":"","modelSize":"480B(激活 35B)","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-25","title":"Coze / Coze Loop","text":"字节跳动（ByteDance）开源的Coze和Coze Loop是AI智能体开发平台，旨在简化和加速AI应用的开发和部署。这些平台提供了一系列工具和组件，帮助开发者快速构建和迭代AI智能体。","modelSize":"N/A (框架)","modelType":"框架","openSource":true,"contextWindow":"N/A","officialDoc":"https://www.coze.com/"},{"date":"2025-07-23","title":"Qwen3-Coder","text":"Qwen3-Coder 是通义千问最新开源的智能体代码模型，在智能体编程、智能体浏览和基础编程任务中表现出色。它具有高度智能体化特性，旨在大幅提升开发效率和代码质量，其性能可与Claude Sonnet媲美，并支持工具调用。","modelSize":"450B(激活 35B)","modelType":"代码生成 (智能体编程)","openSource":true,"contextWindow":"256K (可扩展至1M)","officialDoc":"https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct"},{"date":"2025-07-22","title":"阿里巴巴 Qwen3-Coder-480B-A35B-Instruct","text":"","modelSize":"","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-21","title":"阿里巴巴 Qwen3-235B-A22B-Instruct-2507","text":"","modelSize":"","modelType":"代码","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-14","title":"RoboBrain 2.0 / RoboOS 2.0","text":"智源研究院（BAAI）发布的RoboBrain 2.0和RoboOS 2.0是面向机器人的下一代操作系统和大脑。它们旨在为机器人提供更强的感知、决策和执行能力，推动机器人技术的发展。","modelSize":"N/A (框架)","modelType":"智能体框架","openSource":true,"contextWindow":"N/A","officialDoc":""},{"date":"2025-07-11","title":"KAT-V1-40B","text":"快手（Kuaishou）发布的KAT-V1-40B是一款400亿参数的文本模型，旨在解决复杂推理中的“过度思考”问题。它通过优化推理路径，提升了模型的效率和准确性。","modelSize":"40B","modelType":"语言模型","openSource":true,"contextWindow":"","officialDoc":""},{"date":"2025-07-11","title":"⭐Kimi K2","text":"月之暗面（Moonshot AI）发布的Kimi K2是一款万亿级参数的MoE模型，拥有320亿激活参数。它在长文本处理方面表现卓越，支持128k的上下文长度，并显著提升了中文能力。","modelSize":"1T(激活 320B)","modelType":"语言模型","openSource":true,"contextWindow":"128k","officialDoc":"https://huggingface.co/moonshot-ai/Kimi-K2-32B"},{"date":"2025-07-04","title":"MOSS-TTSD-v0.5","text":"复旦大学（Fudan University）发布的MOSS-TTSD-v0.5是一款基于Qwen3-1.7B-base的双语语音合成模型。它支持长达960秒的语音生成，并在音色、韵律和自然度方面表现出色。","modelSize":"1.7B","modelType":"多模态 (语音生成)","openSource":true,"contextWindow":"960秒","officialDoc":"https://huggingface.co/fnlp/MOSS-TTSD-v0.5"}]